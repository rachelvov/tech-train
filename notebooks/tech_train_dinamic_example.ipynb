{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "pd.set_option('max_colwidth', 10000)\n",
    "pd.options.mode.chained_assignment = None\n",
    "df_dev = pd.read_csv(\"../data/MedQA_dev.csv\", index_col=0)\n",
    "df_train = pd.read_csv(\"../data/MedQA_train.csv\", index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using Azure endpoint:\n",
    "import openai\n",
    "import os\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = os.getenv(\"OPENAI_API_BASE\")\n",
    "openai.api_version = \"2023-07-01-preview\"\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "from tech_train_functions import call_open_ai\n",
    "client = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's focus on question 7 and call it the hard study case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_study_case = df_dev.iloc[9]\n",
    "pd.DataFrame(hard_study_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try using BM25 search to find the most similar question to this in the test split.\n",
    "First let's create BM25 search DB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating BM25 search DB:\n",
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "\n",
    "questions_corpus=df_train['question'].tolist()\n",
    "tokenized_question_corpus = [doc.split(\" \") for doc in questions_corpus]\n",
    "bm25_questions = BM25Okapi(tokenized_question_corpus)\n",
    "\n",
    "def search_similar(query, bm25_index):\n",
    "    query=query.lower()\n",
    "    tokenized_query = query.split(\" \")\n",
    "    doc_scores = bm25_index.get_scores(tokenized_query)\n",
    "    index=np.argsort(-doc_scores)[:2]\n",
    "    return df_train.iloc[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tech_train_functions import analyze_TA4H\n",
    "ta4h_endpoint=os.getenv(\"TA4H_ENDPOINT\")\n",
    "key=os.getenv(\"TA4H_KEY\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done! let's search for closest question to our example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar_in_question = search_similar(hard_study_case[\"question\"], bm25_questions)\n",
    "pd.DataFrame(most_similar_in_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we used the answer options only for the search db?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options_corpus=df_train['options'].astype(str).tolist()\n",
    "tokenized_options_corpus = [doc.split(\" \") for doc in options_corpus]\n",
    "bm25_options = BM25Okapi(tokenized_options_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar_in_answer = search_similar(str(hard_study_case[\"options\"]), bm25_options)\n",
    "pd.DataFrame(most_similar_in_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we used search only by relevant entities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hard_case_entities = analyze_TA4H(ta4h_endpoint, key, [hard_study_case[\"question\"]])[0]\n",
    "hard_case_entities = [e[\"text\"] for e in hard_case_entities if e[\"category\"] in [\"SymptomOrSign\",\"Diagnosis\"] ]\n",
    "hard_case_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar_in_entities = search_similar(\" \".join(hard_case_entities), bm25_questions)\n",
    "pd.DataFrame(most_similar_in_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if these examples are better than a random one!\n",
    "First let's create the CoT example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
