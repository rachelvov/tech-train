{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "print(\"Great! It worked!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = os.getenv(\"OPENAI_API_BASE\")\n",
    "openai.api_version = \"2023-07-01-preview\"\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "from tech_train_functions import call_open_ai\n",
    "client = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_deployment_name = \"gpt-35-turbo\" #if your deployment has different name please change to the correct one\n",
    "print(call_open_ai(system_message=\"You're an AI assistant\", text=\"please write names of 3 animals\", engine=your_deployment_name))\n",
    "print(call_open_ai(system_message=\"You're an AI assistant\", text=\"please write names of 4 fruits\", engine=your_deployment_name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You got the fruits and animals? Great!\n",
    "Now TA4H check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tech_train_functions import analyze_TA4H\n",
    "ta4h_endpoint=os.getenv(\"TA4H_ENDPOINT\")\n",
    "key=os.getenv(\"TA4H_KEY\")\n",
    "\n",
    "documents = [\n",
    "        \"\"\"\n",
    "        low measurment in growth hormone.\n",
    "        \"\"\"\n",
    "    ]\n",
    "result = analyze_TA4H(ta4h_endpoint, key, documents)\n",
    "print([e[\"normalized_text\"] if e[\"normalized_text\"] else e[\"text\"].lower() for e in result[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Got your text Analyzed? Great! Step 1 done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Load Dataset and run first iteration\n",
    "Load MedQA dev split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "pd.set_option('max_colwidth', 10000)\n",
    "pd.options.mode.chained_assignment = None\n",
    "df_dev = pd.read_csv(\"../data/MedQA_dev.csv\", index_col=0)\n",
    "df_train = pd.read_csv(\"../data/MedQA_train.csv\", index_col=0)\n",
    "df_dev.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create small sample for experiments:\n",
    "df_dev_sample = df_dev[:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run our first inference!\n",
    "\n",
    "Here we're supplying GPT with questions, options for answers, and that's all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You're a medical expert answering medical questions. \\\n",
    "Answer with only the correct answer and nothing else, not even the index of the answer.\"\n",
    "sample_list = (df_dev_sample[\"question\"]+\" \"+\n",
    "                df_dev_sample[\"options\"].astype(str)).tolist()\n",
    "print(\"Input example:{}\".format(sample_list[0]))\n",
    "\n",
    "\n",
    "replies = []\n",
    "for q in tqdm(sample_list):\n",
    "    ans = call_open_ai(system_message, q, client=client)\n",
    "    replies.append(ans)\n",
    "df_dev_sample.loc[:,\"reply\"] = replies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check our answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev_sample.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see it's not exactly like the gold answer... we'll be using a fuzzy  match function to assess if the reply is similar to gold answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tech_train_functions import is_similar\n",
    "df_dev_sample.loc[:,\"is correct reply\"] = df_dev_sample.apply(lambda row:is_similar(row[\"answer\"], row[\"answer_idx\"], row[\"reply\"]), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what is our accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev_sample[\"is correct reply\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better than 25% random but not too good... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "## STEP 3 - Chain of Thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message_CoT = \"You're a medical expert answering medical questions. Please answer step by step. Start with 'let's take it step by step'\\\n",
    "and end with 'the correct answer is:' and then the correct answer. Please allways end with this phrase\"\n",
    "\n",
    "sample_list = (df_dev_sample[\"question\"]+\" \"+\n",
    "                df_dev_sample[\"options\"].astype(str)).tolist()\n",
    "print(\"Input example:{}\".format(sample_list[0]))\n",
    "\n",
    "\n",
    "replies = []\n",
    "for q in tqdm(sample_list):\n",
    "    ans = call_open_ai(system_message_CoT, q, client=client)\n",
    "    replies.append(ans)\n",
    "df_dev_sample.loc[:,\"reply_CoT\"] = replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's look at the result:\n",
    "df_dev_sample[\"reply_CoT\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need a different is_similar function now, to get the correct string from the long answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_similar_cot(gold_answer, gold_answer_idx, answer):\n",
    "        if \"correct answer is\" not in answer:\n",
    "              answer = answer.split(\")\")[-1]\n",
    "        answer = answer.split(\"correct answer is\")[-1]\n",
    "        if len(answer) == 1:\n",
    "            return 1 if gold_answer_idx  == answer else 0\n",
    "        gold_answer = gold_answer.lower()\n",
    "        answer = answer.lower()\n",
    "        return 1 if gold_answer in  answer or answer in gold_answer else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev_sample[\"is correct reply CoT\"] = df_dev_sample.apply(lambda row:is_similar_cot(row[\"answer\"], row[\"answer_idx\"], row[\"reply_CoT\"]), axis=1)\n",
    "df_dev_sample[\"is correct reply CoT\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It got worse :-(\n",
    "Let's create an example! Note what dataset we're using..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tech_train_functions import create_example_CoT\n",
    "question,answer,options = df_train.iloc[0][\"question\"],df_train.iloc[0][\"answer\"],df_train.iloc[0][\"options\"]\n",
    "system_message_explainer = \"Please explain step by step how to answer this question.\\\n",
    "Start with 'let's take it step by step' and end with 'the correct answer is:' and then the correct answer idx and the correct answer.\\\n",
    "First examine the case, and suggest treatments without considering the 4 options you got. Only then, return to the options and decide wich is best\"\n",
    "\n",
    "example = create_example_CoT(system_message_explainer,question,options,answer, client=client)\n",
    "print(df_train.iloc[0][\"question\"])\n",
    "print(example[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.iloc[0][\"question\"])\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running again with CoT example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message_CoT = \"You're a medical expert answering medical questions. Please answer step by step. Start with 'let's take it step by step'\\\n",
    "and end with 'the correct answer is:' and then the correct answer. Please allways end with this phrase\"\n",
    "\n",
    "sample_list = (df_dev_sample[\"question\"]+\" \"+\n",
    "                df_dev_sample[\"options\"].astype(str)).tolist()\n",
    "print(\"Input:{}\".format(sample_list[0]))\n",
    "print(\"Example:{}\".format(example[1]))\n",
    "\n",
    "\n",
    "replies = []\n",
    "for q in tqdm(sample_list):\n",
    "    ans = call_open_ai(system_message_CoT, q, examples=[example], client=client)\n",
    "    replies.append(ans)\n",
    "df_dev_sample.loc[:,\"reply_CoT\"] = replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check if we improved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev_sample[\"is correct reply CoT\"] = df_dev_sample.apply(lambda row:is_similar_cot(row[\"answer\"], row[\"answer_idx\"], row[\"reply_CoT\"]), axis=1)\n",
    "df_dev_sample[\"is correct reply CoT\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better! See if you can improve Further by changing CoT prompt or adding additional examples. Don't forget to re-create the example accordingly. You can also try adding more examples and see if it improves the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For test time:\n",
    "\n",
    "question = <insert question here>\n",
    "options = <insert question here>\n",
    "q = question+ \" \"+str(options)\n",
    "call_open_ai(system_message_CoT, q, examples=[example], client=client)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
